// TODO all source code with tabs

[preface]
About the Author
----------------

*Tomek Kaczanowski* is a technical team leader from Krakow, Poland. He has a strong interest in code quality, testing and automation - preferably all three together. Combining technical with soft skills, he also ventures into the realms of mentoring, teaching, lecturing and article writing, not to mention preaching sermons to the unconverted in the hope of redeeming them (or at least their code)! He hates doing things manually, and is allergic to empty +src/test/java+ directories.

Tomek believes that by working with legacy code, and improving it, he can make the world a better place. To his disappointment, the world does not seem to care all that much about his efforts.

Apart from all this weirdness, he is a pretty normal person â€“ a husband, father of two, and cat owner.

Work in progress
----------------
ifndef::print[]
image::images/72/work_in_progress.png[width="200", float="right"]
endif::print[]
ifdef::print[]
image::images/600/work_in_progress.png[width="200", float="right"]
endif::print[]
The book has not been proofreaded (yet)! There are probably many typos and many language mistakes (my apologies!).

THIS IS WORK IN PROGRESS! Whenever you see a *TODO* word it means that this particular part/sentence/paragraph is not finished.

== Why Bother?
This book is not meant to explain all the reasons of writing tests but there are few things we should know. In short, there are three main reasons for writing tests{empty}pass:[]footnote:[Please forgive the simplifications - I want to make it short!]:

* If we have a complete set of tests, and they all pass, then we know that our application works.
* Making changes is much easier if we have tests. They will inform us if we break some functionality.
* Tests are probably the only documentation we have which stays up-to-date during the whole life of ours software.

You should recall these reasons for writing tests when looking at the examples of *bad* tests presented in this book. Ask questions like '"does it test anything?"', '"is it maintainable"', '"is it easy to understand?"' etc.

The worrying thing about writing tests is that there are numerous accounts of people who introduced tests to their development process... and failed! Why? I guess mainly because they lacked knowledge and understanding on how to write good tests. Tests, which are an asset and not a burden.

TIP: Whatever you do, do it good. If it is worth doing, it is worth doing well. Remember this phrases as you write tests!

== No Assertions
These few lines of code are copied from a very old and horrible (truly horrible!) test. Among other things, it had no assertions. Nothing. Null. Zero. Instead, it printed a lot of information to the +System.out+ so that developers could verify whether the tested functionality works or not.

----
IResult result = format.execute();
System.out.println(result.size());
Iterator iter = result.iterator();
while (iter.hasNext()) {
	IResult r = (IResult) iter.next();
	System.out.println(r.getMessage());
}
----
This is of course wrong. Tests should never require us to do any kind of manual intervention. No logs browsing, no querying databases. Nothing like this. All should happen automatically. A good test fails when the tested functionality is malfunctioning.

A better version is presented below. Obviously I have no clue what are the expected values, so I made it up.
----
IResult result = format.execute();
assertThat(result.size()).isEqualTo(3);
Iterator iter = result.iterator();
while (iter.hasNext()) {
	IResult r = (IResult) iter.next();
	assertThat(r.getMessage()).contains("error");
}
----

////
== Flickering Tests
Flickering tests are a serious disease which can be very hard to cure. Once you caught it you can spend days trying to figure out what is going on. One time your tests are [green]#green#, next time they are [red]#red#, then everything is back to normal, but after some time the situation repeats - without any code changes which could possibly cause such result!

This can really turn your hair grey! Usually, such indeterministic behaviour is related to some external systems your tests rely on. Perhaps some 3rd party webservice goes bananas from time to time and responds with nonsensical data. Or maybe there is some subtle bug in the time-related logic of your system which makes your test fail occasionally. 

But sometimes it is our lack of thoughtfullness which can bring that doom on ourselves. At least its mild form - a test which suddenly turns from [green]#green# to [red]#red# without any significant code change (and probably stays like this, which makes it much simpler to debug than the real flickering test).
////

== Autogeneration
Unit tests are simple, right? All they do is they set some object, call its methods, and verify results, right? So, why don't we make computers write them? Hip-hip hurray, we will save a plenty of time, and have 100% code coverage! Let's do it!

...well, it simply does not work, you know. If you think about *why* you really write tests, you will quickly understand that this is a bad idea. It does not help to discover bugs, it does not help you come with better design, it promotes test-last coding and goes against '"test behaviour not methods"' rule (see <<sec_test_behaviour>>). Which really mean, that you should not do it. Howgh!

Below is an attempt to autogenerate some test code (using JUnitDoclet tool{empty}pass:[]footnote:[Nope, not link to project website this time, because I do not think you should use it. :)]). As you can see it successfully generated tests for getters/setters, which is, shortly speaking, a waste of time.

----
public void testSetGetTimestamp() throws Exception {
    // JUnitDoclet begin method setTimestamp getTimestamp
    java.util.Calendar[] tests = {new GregorianCalendar(), null};

    for (int i = 0; i < tests.length; i++) {
    	adapter.setTimestamp(tests[i]);
    	assertEquals(tests[i], adapter.getTimestamp());
    }
    // JUnitDoclet end method setTimestamp getTimestamp
}

public void testSetGetParam() throws Exception {
    // JUnitDoclet begin method setParam getParam
    String[] tests = {"a", "aaa", "---", "23121313", "", null};

    for (int i = 0; i < tests.length; i++) {
    	adapter.setParam(tests[i]);
    	assertEquals(tests[i], adapter.getParam());
    }
    // JUnitDoclet end method setParam getParam
}
----
So now repeat after me: '"I will not autogenerate neither the test code, nor its skeleton. I will not autogenerate neither the test code, nor its skeleton. I will not autogenerate neither the test code, nor its skeleton. I will not autogenerate neither the test code, nor its skeleton..."' (pretty useful mantra if you can't fall asleep, should help after 1000 repetitions).

TIP: Do *NOT* autogenerate neither the test code, nor it skeleton.

== It is a Full Time Job
Let's face it - writing tests makes sense only if you (and your team) take care about them every day. In this section I present two examples of what happens when it is no so.

First is a short snippet of +SystemAdminSmokeTest+ class. *Smoke test*, mind you, which means this is an important test that intends to give you quick feedback about the health of the system. This test at the time I joined this project looked as follows:

----
class SystemAdminSmokeTest extends GroovyTestCase {

void testSmoke() {
// do not remove below code
// def ds = new org.h2.jdbcx.JdbcDataSource(
//     URL: 'jdbc:h2:mem:test;DB_CLOSE_DELAY=-1;MODE=Oracle',
//     user: 'sa', password: '')
//
//     def jpaProperties = new Properties()
//     jpaProperties.setProperty(
//			'hibernate.cache.use_second_level_cache', 'false')
//     jpaProperties.setProperty(
//			'hibernate.cache.use_query_cache', 'false')
//
//     def emf = new LocalContainerEntityManagerFactoryBean(
//        dataSource: ds, persistenceUnitName: 'my-domain',
//        jpaVendorAdapter: new HibernateJpaVendorAdapter(
//            database: Database.H2,  showSql: true,
//            generateDdl: true), jpaProperties: jpaProperties)

   ... 
}
----
Well, it is not gonna inform anyone about the state of the system... I guess there was a time when this test used to work. But then some changes came and it was commented out (I bet it was commented out only '"for a very short time"'....) and it was never brought to life again. A pity, yes. And a rather discouraging sign to anyone joining the team{empty}pass:[]footnote:[Which was misleading BTW, because the project and the team were doing pretty well!].

Another example is even more depressing. In one of the modules of some project I found this test in +src/test/java+ directory:

----
@Test
public class ExampleTest {

	public void testExample() {
		assertTrue(true);
	}
}
----
I think you've already guessed the end of this story. Yes, that was the only test there...

And what can we learn from these two failures? Few things:

* Do not live with '"broken windows"'{empty}pass:[]footnote:[See http://c2.com/cgi/wiki?FixBrokenWindows]; fix things immediately!
* Writing and maintaining tests is an effort which has to be taken by the whole team (and this effort must be also supported, or at least not hindered, by all stakeholders).
* Good intentions are not enough. You also need knowledge, time and determination to meet the goal of well-tested software.

== Do Not Modify Global State
For unit tests a rule of thumb is to keep them independent from each other. It is not so bad if we introduce a dependency on purpose (and explicitly declare it e.g. using TestNG +dependsOnMethod+ feature). A worse scenario is when we are not aware of the dependency ourselves. This can bring serious problems on our heads.

TIP: Keep your tests independent from each other. Or at least make the dependency explicit!

Let us have a look at the following example. The test presented below verifies whether log4j{empty}pass:[]footnote:[http://logging.apache.org/log4j/] configuration code works fine. If some system property (+logConfig+) is available, it should load file specified by this property. If the property is not set it should load some default values.

----
LoggingPropertyConfigurator configurator 
	= mock(LoggingPropertyConfigurator.class);
BaseServletContextListener baseServletContextListener 
	= new BaseServletContextListener(configurator);

@Test
public void shouldLoadDefaultProperties() {
	baseServletContextListener.contextInitialized(null);
	verify(configurator).configure(any(Properties.class));
}

@Test(expected = LoggingInitialisationException.class)
public void shouldThrowLoggingException() {
	System.setProperty("logConfig", "nonExistingFile");
	baseServletContextListener.contextInitialized(null);
}
----
This test used to be [green]#green# for months. Then, suddenly, it turned [red]#red#. Why?! For no apparent reason, the log4j config was not something we changed often. In fact, we haven't touched it for a long time...

After some digging we have found out that the test failed because the order of execution of tests changed. As long as +shouldLoadDefaultProperties+ was executed before +shouldThrowLoggingException+ everything was fine. But once this order was changed, things started to go wrong. In particular, the +logConfig+ system property was available when +shouldLoadDefaultProperties+ test was executed which altered the behaviour of SUT and made this test fail.

And why did the order of execution changed? Well, it does not really matter. In general test frameworks do not guarantee the order of execution (unless you explicitly ask for it) so you should not rely on this.

TIP: Be extra cautious when modifying 'global state' (system properties, file system, database etc.) in your tests. This can influence other tests. Take care to make sure that tests execute in well-defined environment!

Now, how to fix it. Basically there are two solutions.

First, you can impose strict order of execution of these two test methods (TestNG will allow you to do this, JUnit won't).

[role="pseudocode"]
----
@Test
public void shouldLoadDefaultProperties() { ... }

@Test(expected = LoggingInitialisationException.class,
	dependesOnMethod = "shouldLoadDefaultProperties")
public void shouldThrowLoggingException() { ... }
----

This works as long as there are no more tests which outcome also depend on the value of +logConfig+ system property. If such tests are added you need to remember to also specify their relation (their dependency) to these existing tests.

Another option is to clean +logConfig+ system property variable before +shouldLoadDefaultProperties+ is executed. If there are more tests like this then maybe putting the 'cleaning' code some +setUp()+ method would be a good idea. For example:

[role="pseudocode"]
----
@BeforeMethod
public void cleanSystemProperties() {
	System.setProperty("logConfig", null);
}

// the rest of the code remains unchanged
----

TIP: Clean the environment *before* tests, not after it. This way your tests are guaranteed to run in clean environment. 

== Mock'em All!
[quote, Abraham Maslow]
_______________________
To a man with a hammer everything looks like a nail.
_______________________
This is something I observe frequently among developers who has only recently discovered the pleasures of mocking. :) They tend to overuse all kind of test doubles even when it is not really the best option. Let us see an example.

The code below intends to verify whether some data is added to +modelAndView+ object{empty}pass:[]footnote:[This is +ModelAndView+ from Spring MVC framework.]. Yes, you heard it right: it verifies whether something is *added*. Even the method name states this explicitly.

----
@Test
public void shouldAddTimeZoneToModelAndView() {
	//given
	UserFacade userFacade = mock(UserFacade.class);
	ModelAndView modelAndView = mock(ModelAndView.class);
	given(userFacade.getTimezone()).willReturn("timezone X");

	//when
	new UserDataInterceptor(userFacade)
		.postHandle(null, null, null, modelAndView);

	//then
	verify(modelAndView).addObject("timezone", "timezone X"); 
}
----
This is of course wrong. The thing we really care about is whether +modelAndView+ contains certain data, not whether some methods of +modelAndView+ were called!

Below you can see an improved version of this test. It treates SUT more like a black box and only cares about its output.

[role="pseudocode"]
----
@Test
public void shouldAddTimeZoneToModelAndView() {
	//given
	UserFacade userFacade = mock(UserFacade.class);
	ModelAndView modelAndView = new ModelAndView();
	given(userFacade.getTimezone()).willReturn("timezone X");

	//when
	new UserDataInterceptor(userFacade)
		.postHandle(null, null, null, modelAndView);

	//then
	assertThat(modelAndView).contains("timezone", "timezone X");
}
----
As you can see in the second version we use real object, not a test double. And this is fine, because +ModelAndView+ is not a service, but a value object, which should not be mocked. Howgh!

TIP: Excited that you can mock everything, huh? Slow down and make sure that you really need to verify interactions. Chances are you don't.

[[sec_time_means_troubles]]
== Time Means Troubles (Always!)
TIP: As a rule *never* use +System.currentTimeMillis()+ or +new Date()+ in your production code. Add an additional layer of abstraction - e.g. a +DateTimeProvider+ interface will let you test your time-dependent methods with ease.

Let us see what happens when you do not follow this advice.

An obvious example would look like this:

[role="pseudocode"]
----
time = System.currentTimeMillis();

if (time.isAfter(5, PM)) {
	... do some afternoon activity
}
else {
	... do somethig else
}
----
Well, this one is obvious - you can't test all paths util you control the value of time variable.

But let us see a real, slightly more subtle example. Let us take a look at this fragment of production code (the +Util+ class):

----
public String getUrl(User user, String timestamp) {
	String name=user.getFullName();

	String url=baseUrl
		+"name="+URLEncoder.encode(name, "UTF-8")
		+"&timestamp="+timestamp;
	return url;
}

public String getUrl(User user) {
	Date date=new Date();
	Long time=(date.getTime()/1000); //convert ms to seconds
	String timestamp=time.toString();
	return getZendeskUrl(user, timestamp);
}
----

Testing of such methods should be straightforward - they return some +String+ which we could analyze and verify its correctness. Alas, because the +new Date()+ was used, the programmer responsible for this code was not able to verify the resulting URL. Instead, he came up with the following test:

----
@Test
public void shouldUseTimestampMethod() {
        //given
	Util util = new Util();
        Util spyUtil = spy(util);

        //when
        spyUtil.getUrl(user);

        //then
        verify(spyUtil).getUrl(eq(user), anyString());
}
----
Oh my, this is bad for few reasons. This test:

* tests implementation instead of behaviour (see <<sec_test_behaviour>>),
* uses partial mocking (+spy+ method on real object) which is rarely required (a [red]#red# lamp should start blinking in your head when you see it),
* verifies interactions instead of returned values.

And all of this because the production code does not handle properly time!

TIP: See <<sec_fix_the_code_first>> for some more examples of writing tests instead of fixing the code first!

After we redesign the production code by replacing call to +new Date()+ with a call to some collaborator, our test starts looking pretty nice:

----
@Test
public void shouldAddTimestampToGeneratedUrl() {
	//given
	Util util = new ....
	TimeProvider timeProvider = mock(TimeProvider.class);
	when(timeProvider.getTime()).thenReturn("12345");
	util.set(timeProvider);

	//when
	String url = util.getUrl(user);

	//then
	assertThat(url).contains("timestamp=12345");
}
----

TIP: If tested method return some values then try to use this values to verify whether the method works fine. Use mocks/spies only if it is really required{empty}pass:[]footnote:[Please note that the +shouldAddTimestampToGeneratedUrl+ method does not use mocks but a stub only!].

[[sec_srp]]
== SRP for Tests
We are all familiar with Single Responsibility Principle{empty}pass:[]footnote:[SRP, see http://en.wikipedia.org/wiki/Single_responsibility_principle], which basically says that every class should take care of one thing. I think it is very valuable to think about tests in terms of SRP. In case of tests, each test method should conform to the following rule: '"Each test method should verify one scenario"'.

Why? Because:

* such test methods are pretty simple to understand,
* if they fail you know *exactly* which functionality of your software does not work.

NOTE: Writing tests which fullfil the SRP principle is very simple for unit tests, but not always applicable for other kind of tests.

Still, I observe it being breached frequently. Let us have a look at an example. It comes from a simple utility class, which was responsible for making sure that the phone prefix entered by users are valid.

----
@DataProvider
public Object[][] data() {
	return new Object[][] { {"48", true}, {"+48", true}, 
		{"++48", true}, {"+48503", true}, {"+4", false},
		{"++4", false}, {"", false},
		{null, false}, {"  ", false}, };
}

@Test(dataProvider = "data")
public void testQueryVerification(String query, boolean expected) {
	assertEquals(expected, FieldVerifier.isValidQuery(query));
}

----
At the first sight, it may seem like the test is really focused on one thing (on the verification of query validity), but if you look closer you will see some alarming signals:

* its method name is quite generic, and it would be hard to use a '"should"' prefix (see <<sec_should>>),
** also the name of data provider - +data+ - smells really bad,
* it has some kind of logic: assertions depends on passed +expected+ boolean flag,
* a "generic" assertion - +assertEquals+ - is used to verify the outcome.

//(see <<sec_no_logic>>).

To discover if your test fulfills SPR ask the following question: '"if it fails will I be able to discover what fuctionality of my software is broken just by reading the name of the failed test method?"'.

Here is the same tests (meaning, test which verifies exactly the same scenarios) but divided into two parts:

----
@DataProvider
public Object[][] validQueries() {
	return new Object[][] { {"48"}, {"48123"}, 
		{"+48"}, {"++48"}, {"+48503"}};
}

@Test(dataProvider = "validQueries")
public void shouldRecognizeValidQueries(String validQuery) {
	assertTrue(FieldVerifier.isValidQuery(validQuery));
}

@DataProvider
public Object[][] invalidQueries() {
	return new Object[][] { 
		{"+4"}, {"++4"}, 
		{""}, {null}, {"  "} };
}

@Test(dataProvider = "invalidQueries")
public void shouldRejectInvalidQueries(String invalidQuery) {
	assertFalse(FieldVerifier.isValidQuery(invalidQuery));
}
----
This version is longer, but more readable, easier to understand (no boolean flag), and it follows the SRP rule. What I really like here are the names - all of them describe very well the purpose of elements. We have a data provider which provides +validQueries+, and a test method which takes +validQuery+ as a parameter.

TIP: Watch the method names. Are they intention-revealing?

TIP: No logic in tests! Even the simplest one is evil!

// TODO would be nice to have some other examples of this - not everyone uses data providers / parameterized tests

And now, a counter example. Yes, after I have tried to convince you that it is good to split tests into smaller ones (each testing a unique test case), I will show an example of the opposite approach.

Take a look at the test method below:

----
@Test
public void shouldRecognizeSameCampaign() {
	//given
	Campaign otherCampaign = mock(Campaign.class);
	Campaign campaign = mock(Campaign.class);

	//when
	CampaignBid cBid = new CampaignBid(campaign, VALID_BID);

	//then
	assertThat(cBid.hasSameCampaign(otherCampaign)).isFalse();
	assertThat(cBid.hasSameCampaign(campaign)).isTrue();
}
----

It verifies two things, namely:

* whether +CampaignBid+ is capable of recognizing the same domain (which it was constructed with - I haven't presented the constructor, but it takes +Domain+ as one of parameters),
* whether +CampaignBid+ is capable of recognizing domains which are different from the one it was constructed with.

Should we split it in two tests then? My answer is no.

In contrast to the previous example, there are not many cases to test here, so the test method is very concise and simple to understand. In other words, the size of this method (taking into account the number of test cases it verifies, not only number of lines it has) has not exceeded my private safety limit. And thus, I do not feel like having two testing methods instead.

Let us have a look at two-methods version for comparison:

----
@Test
public void shouldRecognizeSameCampaign() {
	//given
	Campaign campaign = mock(Campaign.class);

	//when
	CampaignBid cBid = new CampaignBid(campaign, VALID_BID);

	//then
	assertThat(cBid.hasSameCampaign(campaign)).isTrue();
}

@Test
public void shouldRecognizeDifferentCampaign() {
	//given
	Campaign otherCampaign = mock(Campaign.class);
	Campaign campaign = mock(Campaign.class);

	//when
	CampaignBid cBid = new CampaignBid(campaign, VALID_BID);

	//then
	assertThat(cBid.hasSameCampaign(otherCampaign)).isFalse();
}
----

TIP: Rules are to be broken. ;)

////
another SRP-violating example: http://stackoverflow.com/questions/4152539/how-do-i-test-exceptions-in-a-parameterized-test
////

////
[[sec_no_logic]]
== Logic in Tests
TODO need some good examples of this

avoid by using some features of your testing framework - annotations data provider instead of for loops (TODO ref) and concurrent with annotations

TODO refer to SRP - boolean is also kind of logic
////

== Use Smart Values
Try to minimize the risk of having your tests pass by accident. And always work to improve the readability.

Let us consider the following factory method:

----
public PayoutCalculator create() {
	BigDecimal minMargin = settings.getMinMargin();
	BigDecimal maxMargin = settings.getMaxMargin();
	BigDecimal premiumShare = settings.getPremiumShare();
	return new PayoutCalculator(minMargin, maxMargin, premiumShare);
}
----
+settings+ is a collaborator, and as such should be stubbed in test.

This test does exactly this:

----
public class PayoutCalculatorFactoryTest {
	PayoutSettingsService settings = mock(PayoutSettingsService.class);

	@Test
	public void shouldCreatePayoutCalculator() {
		//given
		given(settings.getMinMargin()).willReturn(new BigDecimal(20));
		given(settings.getMaxMargin()).willReturn(new BigDecimal(50));
		given(settings.getPremiumShare()).willReturn(new BigDecimal(50));

		//when
		PayoutCalculator calculator = new PayoutCalculatorFactory(settings).create();

		//then
		assertThat(calculator)
			.isEqualTo(new PayoutCalculator(new BigDecimal(20), 
			new BigDecimal(50), new BigDecimal(50)));
	}
}
----
Even if the values used in test (+minMargin+ = 20, +maxMargin+ = 50, +premiumShare+ = 50) makes business sense, they should not be used in test. Why? Because there is always a chance, that the tested method is flawed and values are improperly assigned (this can happen if someone created it with *copy&paste* approach). To minimize the risk use different values. The following code shows this (also there is some refactoring done so the values are extracted as +static final+ fields):

----
public class PayoutCalculatorFactoryTest {
	PayoutSettingsService settings = mock(PayoutSettingsService.class);
	
	private static final BigDecimal MIN_MARGIN = new BigDecimal(20);
	private static final BigDecimal MAX_MARGIN = new BigDecimal(30);
	private static final BigDecimal PREMIUM_SHARE = new BigDecimal(40);

	@Test
	public void shouldCreatePayoutCalculator() {
		//given
		given(settings.getMinMargin()).willReturn(MIN_MARGIN);
		given(settings.getMaxMargin()).willReturn(MAX_MARGIN);
		given(settings.getPremiumShare()).willReturn(PREMIUM_SHARE);

		//when
		PayoutCalculator calculator = new PayoutCalculatorFactory(settings).create();

		//then
		assertThat(calculator)
			.isEqualTo(new PayoutCalculator(MIN_MARGIN, MAX_MARGIN, PREMIUM_SHARE);
	}
}
----

TIP: Avoid using same numbers/strings for different variables/properties in your tests method! Often power of 2 are quite convenient (they do not sum up to each other easily).

== Database Assumptions
Take a look at this test:

----
@Test
public void shouldAddAUser() {
	User user = new User();
	userService.save(user);
	assertEquals(dao.getNbOfUsers(), 1);
}
----

Well, the test looks like an integration test (seems like +userService+ and +dao+ talk with real database), which adds a new user to the database.

What is bothering me here is that:

1. it does not really verify if the user was added,
2. it makes some assumptions regarding the state of the database before it is executed,

Regarding the first point, this should be probably done by verifying that the user in the database is equal to the +user+ object. This can be straigforward provided that +userService+ or +dao+ provides appropriate method(s) to fetch the user, or can require much more work (e.g. writing custom JDBC queries directly to database and parsing results in your test code... yuck!).

As for the second issue the solution is quite simple - replace absolute values with relative ones:

----
@Test
public void shouldAddAUser() {
	int nb = dao.getNbOfUsers();
	User user = new User();
	userService.save(user);
	assertEquals(dao.getNbOfUsers(), nb + 1);
}
----
Now the content of the test method reflects its name (+shouldAddUser+). Good.

And BTW. I believe that in many cases such check is good enough to make sure that the user was really added to the database (without really checking whether it is exactly the same user - which should be the case provided that your services and daos are performing simple CRUD operations).

It happened that the assumption on database state (i.e. the assumption that there are no users at all) caused some additional work for my team some time ago. At some point it occured that we need to add a 'fake' user to the system (the reason was to handle some extra-cases, do not show them on reports etc.). After creating a database patch we observerd (with amazement) that many tests started to fail. Why? Because they assumed that the database has no users at all.

TIP: Do not make assumptions about the database content. Use relative rather than absolute values.

TIP: Make sure the test does exactly what its (method) name promises.

== Happy Path
The issue of writing '"happy path"' tests is so common, that it is probably the most popular antipattern of all.

But what is a '"happy path"' testing anyway? For our purpose, it is enough to say that '"happy paths"' tests cover the simplest, even obvious, scenarios. It is like testing that our calculator can add +2+ and +2+, and that if you add user +John Doe+ to the database then you can found it there.

Let me stress one thing though. Such tests are not evil. There is nothing wrong with having them. On the contrary *there are essential*. Why? Because the scenarios they cover are the most important for you application. If your application does not handle such simple cases than it is useless. So the problem is not with writing such tests, but with the fact that they are often the only tests there are. This of course leads to the problems as soon as the real data comes in - suddenly it occurs that your calculator can't handle negative values, and that your database is not prepared to handle a surname longer than 20 characters or so. And your clients will notice it really soon.

TODO what example here?

== Naming is the King
This is interesting because we are usually very good at giving good names to methods and variables in production code. However when it comes to tests we drop the good habits and perform a bad job. I suspect this is because in tests it is quite normal to have few objects of them same type which results in simple naming schemas like +productA+, +productB+, +productC+ etc. Sometimes it is good enough, but sometimes it could be well improved.

The first example presents a simple case of a test related to permission-checking module. The next listing shows a small fragment of this test - a data provider which feeds test methods with data.

----
@DataProvider
public static Object[][] userPermissions() {
 Â  Â return new Object[][]{
 Â  Â  Â  Â {"user_1", READ},
        {"user_2", READ},
Â Â  Â  Â  Â {"user_2", WRITE},
        {"user_3", READ},
Â Â  Â  Â  Â {"user_3", WRITE},
        {"user_3", DELETE}
Â  Â  };
}
----

As you can see it connects three users (+user_1+, +user_2+, +user_3+) with some permissions (+READ+, +WRITE+, +DELETE+). Fine, but why +user_1+ can only +READ+ while +user_3+ can also +WRITE+ and +DELETE+? This is not obvious and if some test fails you will learn someting like: '"+user_3+ was expected to have +DELETE+ permission"'. Then you start scratching your head wondering why actually +user_3+ should have such permission. Of course the answer is nowhere to be found. It was obvious few months ago when this test was created but now? Who knows what the heck is +user_3+?

And the cure? See below.

----
@DataProvider
public static Object[][] userPermissions() {
 Â  Â return new Object[][]{
 Â  Â  Â  Â {"guest", READ},
        {"logged", READ},
Â Â  Â  Â  Â {"logged", WRITE},
        {"admin", READ},
Â Â  Â  Â  Â {"admin", WRITE},
        {"admin", DELETE}
Â  Â  };
}
----

A guest can only read, logged users can also write, and admin has all permissions. Now the error message - e.g. '"+admin+ was expected to have +DELETE+ permission"' makes sense.

Now let us consider another example. This is a (tiny) part of a complex integration test which creates a bunch of objects (mainly of the +Domain+ class). It uses a lot of helper methods which are responsible for creation of objects and also for saving them into the database.

----
Domain domain1 = daoTestHelper.someUtilityMethodWhichCreatesDomain(...)
daoTestHelper.anotherUtilityMethodWhichSetsSomeProperties(domain1, "PL", 1);
daoTestHelper.anotherUtilityMethodWhichSetsSomeProperties(domain1, "US", 1);
Domain domain2 = daoTestHelper.someUtilityMethodWhichCreatesDomain(...)
daoTestHelper.anotherUtilityMethodWhichSetsSomeProperties(domain2, "PL", 4);
daoTestHelper.anotherUtilityMethodWhichSetsSomeProperties(domain2, "US", 8);

// and so on till domain5 or so
----

What I hate about this are the names of domain objects suffixed with numbers (+domain1+, +domain2+ etc.). They bring no information about the state (role, properties) of the objects and make assertions quite mysterious:

----
TODO assertion message here
----

Why do we expect TODO +domain4+ to have exactly TODO? To understand this we need to go back to where it was created and read it line by line. We can do a little better if we use a more descriptive name for our domains. This is shown below.

----
TODO
----

Now the assertion looks better. We expect TODO to have TODO, which seems reasonable.

BTW. While refactoring this test, I have found an interesting thing. Look at the original code once again. Can you see it?

----
TODO
----

Well, maybe you spotted it, but I hadn't when I worked with the code (ok, you knew there is something wrong there, and I hadn't). Only when I started renaming thing I noticed that +domain3+ is used where +domain4+ should had been used. It hasn't influenced the outcome of the test (because this line is responsible for adding data which should not be included in the final result), but still it illustrates very well what happens if all variables names differ only so slightly. It is very simple to mix things up.

TODO img

.Conclusion
It is the same like with classes. At first you begin with useless names like +ClientDAOImpl+ but then you learn that you can have much more intention-revealing names. +HibernateClientDAO+, +InMemoryClientDAO+, +HighPerformanceClientDAO+ or whatever describes this particular implementation. There is always something to say about your class. I believe the same applies to the variables used in tests. I often start with +campaignA+ and +campaignB+ but then it occuress to me that these should be really +campaign+ and +campaignWithSameAdvertiser+ (while in other test method I have +campaign+ and +campaignWithDifferentAdvertiser+). Try it, it makes your test code so much more readable!

[[sec_intention_revealing]]
=== What do True and False Stand For
If we expect our tests to act as a documentation then we need to put some effort into their readability. Consider the following example:

----
server = new MockServer(responseMap, true,
		new URL(SERVER_ROOT).getPort(), false);
----
This single line of code is responsible for creation of some +MockServer+. It is later used in test code.

Nice, but what the heck does +true+ and +false+ mean here? What kind of server are we creating?

Well, I do not expect you to have JavaDocs for +MockServer+ (which sounds like a utility class create especially for testing purposes), which means you need to browse the source code to find out. This is not a major problem (but may be - depending on the complexity of +MockServer+ class), however this is a nuissance. And it means that this test does not play the role of documentation very well. In order to understand the test case we need to browse another documents (i.e. source code). This is not a tragedy, but this is not good either.

And what can we do about this? There are at least two options.

First, we could create some static variables, with intention-revealing names, like this:

----
private static final boolean RESPONSE_IS_A_FILE = true;
private static final boolean NO_SSL = false;

server = new MockServer(responseMap, RESPONSE_IS_A_FILE,
		new URL(SERVER_ROOT).getPort(), NO_SSL);

----
Now there is no doubt what kind of server is created. It does not use SSL and responds with a file. Better, isn't it? And we haven't had to work very hard to achieve this effect.

Second approach requires some more work. Yeah, you guessed it already - we could use a test data builder idea and express very clearly what kind of server is required. For example like this:
----
server = new MockServerBuilder()
		.withResponse(responseMap)
		.withResponseType(FILE)
		.withUrl(SERVER_ROOT)
		.withoutSsl().create();
----
Is this approach better than the previous one? Definitely it requires more work. Probably the more complex, and more frequently created, the class in question is, the more benefit test data builders would bring us.

TODO ref to test data builders chapter

[[sec_should]]
=== Should is Better than Test
Once upon a time, where His Majesty King JUnit the Third ruled the land of Java testing, we were all forced to prefix our test methods with +test+ word. Somehow this custom survived the death of the king{empty}pass:[]footnote:[Some people says it was his younger brother - JUnit the Fourth - who killed him. Others say it was barbarians from TestNG who did this. Whatever. He is dead. Period.]. Which is really troublesome as we will soon see.

// TODO in relation to "test behaviour not methods" <<sec_test_behaviour>>

Take a look at the following example (looks like Groovy judging by the lack of semicolons):
----
@Test
public void testOperation() {
	configureRequest("/validate")
	rc = new RequestContext(parser, request)
	assert rc.getConnector() == null
	assert rc.getOperation().equals("validate")
}
----
Now what is the purpose of this test? The name says it should +testOperation+ which is rather vague. What kind of operation? What behaviour is expected? I do not understand this test, but I think it more about testing of +validate+ requests and not about some +operation+.

The thing is, that the documentational value of this test is zero, or even less. It rather brings confusion than clarifies anything.

As shown by this example the first problem with test method prefixed with +test+ is that they tend to test 'something'.

Take a look at the next example which uncovers another downside of +test+ naming pattern. As the name of this test method says it '"tests query"'. 

----
@Test
public void testQuery(){
	when(q.getResultList()).thenReturn(null);
	assertNull(dao.findByQuery(Transaction.class, q, false));
	assertNull(dao.findByQuery(Operator.class, q, false));
	assertNull(dao.findByQuery(null, null, false));

	List result = new LinkedList();
	when(q.getResultList()).thenReturn(result);
	assertEquals(dao.findByQuery(Transaction.class, q, false), result);
	assertEquals(dao.findByQuery(Operator.class, q, false), result);
	assertEquals(dao.findByQuery(null, null, false), null);

	when(q.getSingleResult()).thenReturn(null);
	assertEquals(dao.findByQuery(Transaction.class, q, true).size(), 0);
	assertEquals(dao.findByQuery(Operator.class, q, true).size(), 0);
	assertEquals(dao.findByQuery(null, null, true), null);

	when(q.getSingleResult()).thenReturn(t);
	assertSame(dao.findByQuery(Transaction.class, q, true).get(0), t);
	when(q.getSingleResult()).thenReturn(o);
	assertSame(dao.findByQuery(Operator.class, q, true).get(0), o);
	when(q.getSingleResult()).thenReturn(null);
	assertSame(dao.findByQuery(null, null, true), null);
}
----
This test:

* definitely breaks SRP (see <<sec_srp>>),
* uses some magical switches (who tells me what +true+ and +false+ stand for? - see <<sec_intention_revealing>>),
* uses cryptic variables name (q+, +o+ and +t+)

but all in all, it is a valuable test. Believe me, I analyzed every part of it, and it all makes some sense. However, I hate the way it looks. It is too long and it verifies too many scenarios.

And here we come to the second problem which often occurs in +test+ prefixed test methods. Once you write something like +testQuery+ it feels natural to put there every test related to query. That is how such monstrous methods are born. Bad thing, really bad.

I'm convinced that this would never have happened if only the developer had started with +should+ prefix. When you type +should...+ it makes you think about some specific scenarios. And you end up with +shouldReturnNullWhenDaoReturnsNull+ or with +shouldReturnSingleValueReturnedByDao+ and so on, but you won't end up with +shouldTestQuery+ unless you are really trying to anger me. ;)

The test would be much better if splitted into few test methods, like this (BTW. as mentioned before I do not think this are the best tests ever written...):

----
public void shouldReturnNullListWhenDaoReturnsNull {
	....
}

public void shouldReturnEmptyListWhenDaoReturnsIt {
	....
}

public void shouldReturnNullSingleResultWhenDaoReturnsNull {
	....
}

public void shouldReturnSingleResultReturnedByDao {
	....
}
----

TIP: Start with +should+. Think about scenario. Do not use +test+ prefix.

[[sec_test_behaviour]]
== Test Behaviour Not Methods
TODO

== Assertions

=== Say No to Complex Assertions
In unit tests assertions are usually not a problem. Your tests are focused and test one thing so there is usually only one assertion and everything is clear. However it is much worse with integration and end-to-end tests in which the assertion part could be huge.

Let us have a look at an example of such situation. This test verifies whether a certain artifact (+WAR+ file) was copied to some remote server.
----
public void shouldPreDeployApplication() {
	// given
	Artifact artifact = mock(Artifact.class);
	when(artifact.getFileName()).thenReturn("war-artifact-2.0.war");
	ServerConfiguration config 
		= new ServerConfiguration(ADDRESS, USER, KEY_FILE, TOMCAT_PATH, TEMP_PATH);
	Tomcat tomcat = new Tomcat(HTTP_TOMCAT_URL, config);
	String destDir = new File(".").getCanonicalPath() + SLASH + "target" + SLASH;
	new File(destDir).mkdirs();

	// when
	tomcat.preDeploy(artifact, new FakeWar(WAR_FILE_LENGTH));

	//then
	JSch jsch = new JSch();
	jsch.addIdentity(KEY_FILE);
	Session session = jsch.getSession(USER, ADDRESS, 22);
	session.setConfig("StrictHostKeyChecking", "no");
	session.connect();
	Channel channel = session.openChannel("sftp");
	session.setServerAliveInterval(92000);
	channel.connect();
	ChannelSftp sftpChannel = (ChannelSftp) channel;

	sftpChannel.get(TEMP_PATH + SLASH + artifact.getFileName(), destDir);
	sftpChannel.exit();

	session.disconnect();

	File downloadedFile = new File(destDir, artifact.getFileName());

	assertThat(downloadedFile).exists().hasSize(WAR_FILE_LENGTH);
}
----
As you can see the 'then' part is pretty huge and rather unpleasant. It contains a lot of implementation details which clutter the view. It is not so simple to read the test and understand the scenario it verifies.

How to make it better? Many of us would extract assertion part as private method. Ok, this would be better, but as we see in a minute (see <<sec_assertions_vs_private_methods>>) it can lead to some problems as well, so let us do something different.

----
public void shouldPreDeployApplication() {
	//given
	Artifact artifact = mock(Artifact.class);
	when(artifact.getFileName()).thenReturn(ARTIFACT_FILE_NAME);
	ServerConfiguration config 
		= new ServerConfiguration(ADDRESS, USER, KEY_FILE, TOMCAT_PATH, TEMP_PATH);
	Tomcat tomcat = new Tomcat(HTTP_TOMCAT_URL, config);

	// when
	tomcat.preDeploy(artifact, new FakeWar(WAR_FILE_LENGTH));

	// then
	SSHServerAssert.assertThat(ARTIFACT_FILE_NAME)
		.existsOnServer(config).hasSize(WAR_FILE_LENGTH);
}
----
Better, isn't it? Now the test code speaks in terms of the requested functionality and not in terms of implementation. Good.

My advice is that when you see the assertion part of your test code growing, you should stop and write the assertion as you would like it to be. Then replace all the current asserting code with this one-liner and try to implement your custom assertion. It is easier then you think (no matter if you use Hamcrest or FEST Fluent Assertions).

TIP: Just say it. Write the assertions as you would like it to be. Then implement it.

Let us see another example of overly complicated assertions.

As you can see below an indexing variable +i+ is used to navigate through the resulting list. Assertions are rather cryptic. At least for me it is not obvious that the third element of this list should have twelve views.
----
@Test
public void shouldGetDomainsFromCampaignWithStatisticsAndWithDeleted() {
	//given
	... some complicated set-up here

	//when
	List<DomainObject> result = someDao.getStatistics(campaign, true, 100, 0);

	//then
	int i = 0;
	assertThat(result.size()).isEqualTo(5);
	assertThat(result.get(i).getCampaignId()).isEqualTo(campaign.getId());
	assertThat(result.get(i++).getViews()).isEqualTo(16);
	assertThat(result.get(i++).getViews()).isEqualTo(12);
	assertThat(result.get(i++).getViews()).isEqualTo(3);
	assertThat(result.get(i++).getViews()).isEqualTo(0);
	assertThat(result.get(i++).getViews()).isEqualTo(0);
}
----

As usual when it comes to making assertions more readable we can use private methods (not really recommended - see <<sec_assertions_vs_private_methods>>) or write our custom assertions. Since in this case the test verifies some very important part of the system, I decided it was worthwile to implement custom assertion. The result is shown on the next listing.

----
@Test
public void shouldGetDomainsFromCampaignWithStatisticsAndWithDeleted() {
	//given
	... some complicated set-up here

	//when
	List<DomainObject> result = someDao.getStatistics(campaign, WITH_EXCLUDED, 100, 0);

	//then
	assertThat(result).hasStatisticsForDomains(5).allStatisticsAreForCampaign(campaign.getId());
	assertThat(result).isSortedByViews();
	assertThat(result).hasResultsForDomain(domainWith3Views, 3);
	assertThat(result).hasResultsForDomain(domainWith12Views, 12);
	assertThat(result).hasResultsForDomain(deletedDomainWith16Views, 16);
	assertThat(result).hasResultsForDomain(domainWith0Views, 0);
	assertThat(result).hasResultsForDomain(deletedDomainWith0Views, 0);
}
----

TODO refer to test data builders - this is the other side of the same coin

TIP: Let your tests (and assertions) speak with domain language, not in implementation terms.

[[sec_assertions_vs_private_methods]]
=== Avoid Assertions using Private Methods
TODO assertions from Martin which were hard to undrestand

So you have a test scenario. It is a complex integration test which covers some important functionality. At the end of it you write some assertions to make sure that the data in the database are in expected state. It can look like this:

----
TODO

test with assertState but inline, and with less assertions.
----

So far so good. Now you write other tests. You soon discover that the assertions are very similar to the ones you have already written. '"Reuse is good"' you think and you do the most obvious thing - you create a +private+ method and call it from both tests (and later from more tests).

And all is good, at least for some time. You write more tests still reusing this one assertion method. But soon you encounter problems. The assertions in the method are not exactly what you need in some particular test cases. They are very close to what you want, but not exactly. '"Reuse is good"' you say and make your private method more generic so it can handle more test cases. The result is presented below:

----
TODO normal assertState() method
----

Oh my, this has come too far... Now we have a monster-method which verifies every possible scenario! It takes TODO almost 10 parameters and has some logic. This is unmaintainable. You can't read your tests now and understand what scenario is really tested (without doing some serious investigations).

And what can be done about it.

* assertions
* setup

TODO rule of a thumb: whenever your assertions are growing beyond your safety limit (2 lines? 3 lines? 5 lines? I do not know, it is *yours safety limit* not mine) then introduce custom assertions instead. In fact, it seems reasonable to come with custom assertions for the main objects from your domain (because you will probably have many tests which involve them - both on unit and integration level).

And how to come with good custom assertions method? My trick is simply to write it so I am happy while reading this. So, without actully having any custom assertion implemented I write a line like this in my test code:

----
assertThat(client).isVip().and().hasDiscount(0.2)
----

And then I implement it. Of course sometimes it happens that I need to deviate from the original plan and that the resulting fluent interface differs in details, but usually I'm able to come quite close to what I imagined.

=== Cost of Custom Assertions
There is a question which could be asked by an inquisitive reader. Custom assertions contain some code, even some logic. How can I be sure that there is no mistake there?

Ok, I admit. I do not test my custom assertions. This comes from few facts.

First of all the logic of custom assertions is *very* limited (if any at all). Usually they boil down to fetching some properties of an object and comparing it with expected result. Writing custom assertions for your domain objects you will rarely find a need for a +for+ loop there or anything more complicated. It often happens that the majority of code is devoted to the creation of custom error messages. +
This looks different for custom assertions which are more related to integration tests - like the one discussed in <<sec_asserting_too_much>>. In such case you can consider writing of tests for your assertions.

Secondly, custom assertions are often written after you have already written assertions in your tests. This is rather natural. At some point of writing your test you realize that the assertion part of it is getting too big and/or too complicated. Then you create your custom assertion class and copy there parts of your assertions. This usually happens when you verified your assertions working (by running tests). 

== Excessive SetUp
TODO merge with previous one?

builders

TODO example from DaoTestHelper - construction of objects with few BigDecimal parameters - show how to replace with constants and builders Problems: hard to understand.

== Things to Remember

* Single Responsibility Principle for tests
* Names are important!
* Test behaviour, not methods!
* Ask yourself "what is the purpose of this test?", "what is really important?"
* Think about readability (just say it)
* Test-last does not work
* Private methods for assertions do not work
* Tests work only if you treat them as first-class citizens

== Know Your Tool
TODO

[[sec_thread_sleep]]
Thread.sleep()
--------------
TODO any example of such things?

* got selenium one - mention it can leverage what frameworks provide
* need something else to introduce awaitility

Test Which Knows Too Much
-------------------------
Why tests are brittle? Because we put too many details into them. See the following example (this is a slightly shorthened test which from some mailing list discussion).
----
public void createSurvey() throws InterruptedException {		
	//CREATE SURVEY
	WebElement allproject = driver.findElement(By.xpath("//*[@id='projectnav']/ul/li[2]/a"));
	allproject.click();
	
	Thread.sleep(5000);
	
	WebElement myfolder = driver.findElement(By.linkText("John Doe"));
	myfolder.click();
		
	Thread.sleep(5000);
		
	WebElement myProject = driver.findElement(By.linkText("My project"));
	myProject.click();
		
	Thread.sleep(5000);
		
	WebElement createsurveylink = driver
		.findElement(By.xpath("//*[@id='bcontrol']/body/form[1]/table[2]/tbody/tr/td[2]/a[1]/img"));
	createsurveylink.click();
				
	Thread.sleep(5000);
		
	WebElement surveyname = driver
		.findElement(By.xpath("//*[@id='bcontrol']/body/form/table[4]/tbody/tr[2]/td[3]/input"));
	surveyname.sendKeys("Test Survey created on " + new Date());
		
	WebElement surveynameconfirm = driver
		.findElement(By.xpath("//*[@id='bcontrol']/body/form/table[1]/tbody/tr[2]/td[3]/a[2]/img"));
	surveynameconfirm.click();
		
	Thread.sleep(5000);
}
----
Before we get to the main point, let us discuss the idea of having +Thread.sleep()+ calls in your tests once again. We already did this in <<sec_thread_sleep>> but here situation is slightly different. The main reason of making the test so sloooooooow is because we are not sure when the browser will finish rendering the page so we can proceed the next click. True, we do not know that, so a defensive strategy is to set up some high values (5 seconds in this case) to stay on the safe side. It works, but makes your tests execution time horrible!

So, what can we do about it?  We can't make browser speed up, but we can wait actively, that is wait and repeatedly verify whether we can move do the next click. TODO about internal selenium mechanisms and about awaitility

TODO about Page objects

So, putting these two together, the resulting test could look like this (it is hard to me to guess what was the business requirement of the test presented above, but I guess it was something like this):

----
public void shouldCreateSurvey() {
	Date date = new Date();

	ProjectsPage projectsPage = mainDashboard.goToProjectsPage();

	projectsPage.openProject("My project");

	SurverEditionPage surverEditionPage = projectsPage.createSurvey("Test Survery created on " + date);

	// there were no assertions in the original tests (which was rather weird...),
	// but I guess something like this would make sense
	String surveyName = surveyEditionPage.getEditedSurveyName();

	assertThat(surveyName).isEqualTo("Test Survey created on " + date);
}
----
What can we see here? Few things:

* most of the work is delegated to various +xyzPage+ objects which handle things like XPath access to various elements,
* no +Thread.sleep()+ - this is also handled internally by +xyzPage+ objects,
* this test would survive refactorings as long as the general flow of actions remains unchanged.

All this does not mean that the nasty low-level details have miracouleously vanished. No, as we already mentioned, they were moved to +xyzPage+ classes. For example, the +ProjectsPage+ class could look like the following:

----
TODO
----

[[sec_asserting_too_much]]
== Asserting Too Much
TODO comparison of objects with +equals+

Take a look at this test. Only the last lines of it are presented, but believe me - it was a really complicated end-to-end test. It involved the SUT (the whole system) receive some requests and generate answers in form of CSV files.

----
public void invalidTxShouldBeCanceled() {
	... some complicated test here

	// then
	String fileContent = 
		FileUtils.getContentOfFile("response.csv");
	assertTrue(fileContent.contains(
		"CANCEL,123,123cancel,billing_id_123_cancel,SUCCESS,"));
}
----
Look at the method name - +invalidTxShouldBeCanceled+ (hint: 'Tx' stands for 'transaction') - and at the assertion made. Hm, do they match? Do you feel like the assertion really verifies if the invalid transaction was canceled? Well, it probably does, but it seem to test much more than this. In fact it verifies if the answer about the outcome of transaction processing was given according to some format of data.

Another variant, which I would recommend, relies on - surprise, surprise! - custom annotations.
----
public void invalidTxShouldBeCanceled() {
	... some complicated test here

	// then
	String fileContent = 
		FileUtils.getContentOfFile("response.csv");
	TxDTOAssert.assertThat(fileContent)
		.hasTransaction("123cancel").withResultCode(SUCCESS);
}
----
I like this version much more. It hides implementation details. Inside custom assertion code there is a logic which handles the file content (and extracts the status from single line of this CSV file) but it is hidden there, and does not clutter the test. It also reads much nicer: 'assert that response file contains entry about the transaction +123cancel+ which says it has finished successfuly (meaning, the cancelation of this transaction succeeded)'. 

== Avoid Overspecified Tests
The code below - copied from Mockito mailing list - is definitely breaking one of the rules of mocking: '"mock only types you own"'. But what it also does is that it is so very detailed that it describes every single bit of the production code (I say it without actually seeing the production code, but I bet this is the case!). This is bad. The test code is so tightly coupled to implementation that you can't introduce any changes there without breaking it.

And what does it really test? Well, everything and nothing at all at the same time. It verifies all the interactions between the SUT and its collaborators, but it even goes further: it stubs the SUT and then it verifies if it was stubbed! This is really useless unless your goal is to test Mockito's stubbing abilities (do not do that, thousands of people has already proved Mockito works fine!).

----
@Mock private DataSource dataSource;

@Mock private Mock connection;

@Mock private Mock statement;

@Mock private ResultSet resultSet;

@Test
public void test() throws Exception {
	MockitoAnnotations.initMocks(this);
	systemUnderTest = new OracleDAOImpl();
	systemUnderTest.setDBConnectionManager(connectionManager);
	Set<NACustomerDTO> set = new HashSet<NACustomerDTO>();
	when(connectionManager.getDataSource()).thenReturn(dataSource);
	when(dataSource.getConnection()).thenReturn(connection);
	when(connection.createStatement()).thenReturn(statement);
	when(statement.executeQuery(anyString())).thenReturn(resultSet);
	when(resultSet.next()).thenReturn(false);
	when(resultSet.getLong(1)).thenReturn(1L);
	when(resultSet.getString(2)).thenReturn("7178");
       
	doNothing().when(resultSet).close();
       
	stub(systemUnderTest.getNACustomers()).toReturn(set); <1>
	final Set<NACustomerDTO> result = systemUnderTest.getNACustomers();
       
 	verify(connectionManager).getDataSource();
 	verify(dataSource).getConnection();
 	verify(connection).createStatement();
	verify(statement).executeQuery(anyString());
	verify(resultSet).next();
	verify(resultSet).getLong(1);
	verify(resultSet).getString(2);
       
	assertNotNull(result); <2>
       
	verify(connectionManager).getDataSource().getConnection();
}
----
<1> Stubbing of the SUT.
<2> This line verifies whether the stubbing was successful.

TIP: Writing test for exisiting code is *NOT* about repeating it line by line!

Ok, that would be unfair to leave this test like this. I think we deserve at least few hints about testing your DAO layer.

The rule of thumb is, that your DAOs are something you should rather not unit test. Why? Because there is usually no logic there (DAO should be a thin wrapper over database stuff). Instead have integration tests which set up a real database. If your DAO is not vendor-specific, it can be any database which is convenient for testig - e.g. H2{empty}pass:[]footnote:[http://www.h2database.com]. Then verify that the entites inserted and fetched by your DAOs are what you expect them to be. Concentrate on the complicated stuff (provided that your DAO layer have such). Do not test simple CRUD functionality that your ORM is doing for you (if there is something wrong here - which is very unlikely - you will find out when running end-to-end tests).

TIP: Go search on http://stackoverflow.com/search?q=dao%20testing[Stack Overflow] for '"dao testing"' - you will find many good tips there.

[[sec_fix_the_code_first]]
== Fix the Code Then Write Tests

example from CW - complicated factoryWhatever test

should fix the code, not try to somehow test it

TODO of course test-first is better

== Randomness
TODO ref Randomized Tests{empty}pass:[]footnote:[http://wiki.apidesign.org/wiki/RandomizedTests] by Jaroslav Tulach
TODO ref RandomizedTesting{empty}pass:[]footnote:[http://labs.carrotsearch.com/randomizedtesting.html] by Carrot Search Labs

<<<
[float]
== Help
Send me your tests!

<<<
[float]
== Practical Unit Testing with TestNG and Mockito
image::images/put_cover.jpg[width="200", float="right"]
If you feel like you want to know more about writing unit tests, mocking, assertions and all this stuff, then have a look at my previous book - http://practicalunittesting.com['"Practical Unit Testing with TestNG and Mockito"'] (2012). You can learn a lot from it!

Learn to write *high-quality unit tests* with the finest technologies of the Java world!

Please visit http://practicalunittesting.com[book's website] for more information.
