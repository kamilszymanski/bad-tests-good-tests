Content
-------
TODO

The book has not been proofreaded! There are probably many typos and many language mistakes (my apologies!).

THIS IS WORK IN PROGRESS!
About TODO - it means some part/sentence/paragraph is not finished

TODO What you should know.

TODO not "bad" but rather "imperfect"

To really benefit from reading this book you should:

* have some experience in writing tests, especially unit tests,
* know a testing framework; preferably TestNG, but any other will also be fine (but you have to scan TestNG documentation to understand code samples),
* be interested in improving your tests-writing skills,
* understand the value of clean test code.

The source I recommend is my book{empty}pass:[]footnote:[See http://practicalunittesting.com] (well, of course!) but to be honest you can learn most of it by doing a research on your own (web has it all, but you have to find it first and it takes time!).

TODO about +setUp()+ what I mean by this

TODO Differences with THE book. Here more strict view, no discussion of possible options, no explanations. Zero or one. Stop or go. White or black. No shades of grey.

== Why Bother?
This book is not meant to explain all the reasons of writing tests but there are few things we should know. In short, there are three main reasons for writing tests{empty}pass:[]footnote:[Please forgive the simplifications - I want to make it short!]:

* If we have a complete set of tests, and they all pass, then we know that our application works.
* Making changes is much easier if we have tests. They will inform us if we break some functionality.
* Tests are probably the only documentation we have which stays up-to-date during the whole life of ours software.

You should recall these reasons for writing tests when looking at the examples of *bad* tests presented in this book. Ask questions like '"does it test anything?"', '"is it maintainable"', '"is it easy to understand?"' etc.

TODO Whatever you do, do it good. If it is worth doing, it is worth doing well. Numerous accounts of people who introduced tests to their development process... and failed!

== Introduction
I would like to start our journey through various tests by going few years back. The tests discussed in this section were written in years 2004-2006. In fact, they are full of atrocities of the worst kind. Any single of them could kill a code-clean acolyte. ;) I've received these tests Bartosz{empty}pass:[]footnote:[You can follow Bartosz at https://twitter.com/#!/bocytko] who mentions some of their properties:

* The tests were written in 2004-2006.
* They were never a part of any serious test harness (i.e. no regular execution on CI server).
* Some tests do not even compile.
* In some tests you can read a comment that "WARNING: test requires the divide param to be set to 20" but the code is so ugly, that there is no way to inject this value.
* Some test data are available in form of serialized objects (+*.ser+) that can not be currently deserialized, because the classes have changed.
* The project is now in maintenance.

[NOTE]
.Cast the first stone...
=====================================================================
The reason we look at these tests is not to have a laugh at developers who have written them. I do not doubt they did the best they could. I wonder myself wheter in 2004 I was able to write better tests, or even whether I was able to write any tests at all... There are two reasons I discuss these ancient tests. Firstly, let us see how much more educated and experienced we (as a community of developers) are in the area of tests. Secondly, there is much to learn even from such imperfect tests.
=====================================================================

Ok, so now let's dive into the dark ages of testing and see what we can learn from it.

=== It is a Full Time Job
TODO thanks Bartosz

Let's face it - writing tests makes sense only if you (and your team) take care about them every day. In this section I present two examples of what happens when it is no so.

First is a short snippet of +SystemAdminSmokeTest+ class. *Smoke test*, mind you, which means this is an important test that intends to give you quick feedback about the health of the system. This test, at the time I joined this project, looked as follows:

----
class SystemAdminSmokeTest extends GroovyTestCase {

void testSmoke() {
// do not remove below code
// def ds = new org.h2.jdbcx.JdbcDataSource(
//     URL: 'jdbc:h2:mem:test;DB_CLOSE_DELAY=-1;MODE=Oracle',
//     user: 'sa', password: '')
//
//     def jpaProperties = new Properties()
//     jpaProperties.setProperty(
//			'hibernate.cache.use_second_level_cache', 'false')
//     jpaProperties.setProperty(
//			'hibernate.cache.use_query_cache', 'false')
//
//     def emf = new LocalContainerEntityManagerFactoryBean(
//        dataSource: ds, persistenceUnitName: 'my-domain',
//        jpaVendorAdapter: new HibernateJpaVendorAdapter(
//            database: Database.H2,  showSql: true,
//            generateDdl: true), jpaProperties: jpaProperties)

   ... 
}
----
Well, it is not gonna inform anyone about the state of the system... I guess there was a time when this test used to work. But then some changes came and it was commented out (I bet it was commented out only '"for a very short time"'....) and it was never brought to life again. A pity, yes. And a rather discouraging sign to anyone joining the team{empty}pass:[]footnote:[Which was misleading BTW, because the project and the team were doing pretty well!].

Another example is even more depressing. In one of the modules of some project I found this test in +src/test/java+ directory:

----
@Test
public class ExampleTest {

	public void testExample() {
		assertTrue(true);
	}
}
----
I think you've already guessed the end of this story. Yes, that was the only test there...

And what can we learn from these two failures? Few things:

* Do not live with '"broken windows"'{empty}pass:[]footnote:[See http://c2.com/cgi/wiki?FixBrokenWindows]; fix things immediately!
* Writing and maintaining tests is an effort which has to be taken by the whole team (and this effort must be also supported, or at least not hindered, by all stakeholders).
* Good intentions are not enough. You also need knowledge, time and determination to meet the goal of well-tested software.

=== Autogeneration
Unit tests are simple, right? All they do is they set some object, call its methods, and verify results, right? So, why don't we make computers write them? Hip-hip hurray, we will save a plenty of time, and have 100% code coverage! Let's do it!

...well, it simply does not work, you know. If you think about *why* you really write tests, you will quickly understand that this is a bad idea. It does not help to discover bugs, it does not help you come with better design, it promotes test-last coding and goes against '"test behaviour not methods"' rule. Which really mean, that you should not do it. Howgh!

Below is an attempt to autogenerate some test code (using JUnitDoclet tool{empty}pass:[]footnote:[Nope, not link to project website this time, because I do not think you should use it. :)]). As you can see it successfully generated tests for getters/setters, which is, shortly speaking, a waste of time.

----
public void testSetGetTimestamp() throws Exception {
    // JUnitDoclet begin method setTimestamp getTimestamp
    java.util.Calendar[] tests = {new GregorianCalendar(), null};

    for (int i = 0; i < tests.length; i++) {
    	adapter.setTimestamp(tests[i]);
    	assertEquals(tests[i], adapter.getTimestamp());
    }
    // JUnitDoclet end method setTimestamp getTimestamp
}

public void testSetGetParam() throws Exception {
    // JUnitDoclet begin method setParam getParam
    String[] tests = {"a", "aaa", "---", "23121313", "", null};

    for (int i = 0; i < tests.length; i++) {
    	adapter.setParam(tests[i]);
    	assertEquals(tests[i], adapter.getParam());
    }
    // JUnitDoclet end method setParam getParam
}
----
So now repeat after me: '"I will not autogenerate nor the test code, neither its skeleton. I will not autogenerate nor the test code, neither its skeleton. I will not autogenerate nor the test code, neither its skeleton. I will not autogenerate nor the test code, neither its skeleton..."' (pretty useful mantra if you can't fall asleep, should help after 1000 repetitions).

=== No assertions, manual verification
TODO

=== Conclusions
TODO 

* it is an everyday effort,
** (vs. follow the leader, and commented out smoke test)
* do not generate, write them!,
* master your tools (or at least learn the basics),
* tests should inform about the reason they failed.

== Flickering Tests
Flickering tests are a serious disease which can be very hard to cure. Once you caught it you can spend days trying to figure out what is going on. One time your tests are [green]#green#, next time they are [red]#red#, then everything is back to normal, but after some time the situation repeats - without any code changes which could possibly cause such result!

This can really turn your hair grey! Usually, such indeterministic behaviour is related to some external systems your tests rely on. Perhaps some 3rd party webservice goes bananas from time to time and responds with nonsensical data. Or maybe there is some subtle bug in the time-related logic of your system which makes your test fail occasionally. 

But sometimes it is our lack of thoughtfullness which can bring that doom on ourselves. At least its mild form - a test which suddenly turns from [green]#green# to [red]#red# without any significant code change (and probably stays like this, which makes it much simpler to debug than the real flickering test).

Let us have a look at the following example. The test presented below verifies whether log4j{empty}pass:[]footnote:[http://logging.apache.org/log4j/] configuration code works fine. If some system property (+logConfig+) is available, it should load file specified by this property. If the property is not set it should load some default values.

----
LoggingPropertyConfigurator configurator 
    = mock(LoggingPropertyConfigurator.class);
BaseServletContextListener baseServletContextListener 
    = new BaseServletContextListener(configurator);

@Test public void shouldLoadDefaultProperties() {
    baseServletContextListener.contextInitialized(null);
    verify(configurator).configure(any(Properties.class));
}

@Test(expected = LoggingInitialisationException.class)
public void shouldThrowLoggingException() {
    System.setProperty("logConfig", "nonExistingFile");
    baseServletContextListener.contextInitialized(null);
}
----
This test used to be [green]#green# for months. Then, suddenly, it turned [red]#red#. Why?! For no apparent reason, the log4j config was not something we changed often. In fact, we haven't touched it for a long time... The reason was, that this test relied on the order of execution. As long as +shouldLoadDefaultProperties+ was executed before +shouldThrowLoggingException+ everything was fine. But once this order was changed, things started to go wrong. In particular, the +logConfig+ system property was available in when +shouldLoadDefaultProperties+ test was executed, which altered the behaviour of SUT and made this test fail.

And why did the order of execution changed? Well, it does not really matter. In general test frameworks do not guarantee the order of execution (unless you explicitely ask for it) so you should not rely on this.

Now, how to fix it. Basically there are two solutions:

* clean +logConfig+ system property variable before +shouldLoadDefaultProperties+ is executed (if you have more test here then maybe putting this into some +setUp()+ method would be a good idea),
* or imopse strict order of execution of these two test methods (TestNG will allow you to do this, JUnit won't).

== Mock'em All!
[quote, Abraham Maslow]
_______________________
To a man with a hammer everything looks like a nail.
_______________________
This is something I observe frequently among developers who has only recently discovered the pleasures of mocking. :) They tend to overuse all kind of test doubles even when it is not really the best option. Let us see an example.

The code below intends to verify whether some data is added to +modelAndView+ object{empty}pass:[]footnote:[This is +ModelAndView+ from Spring MVC framework.]. Yes, you heard it right: it verifies whether something is *added*. Even the method name states this explicitly.

----
@Test
public void shouldAddTimeZoneToModelAndView() {
  //given
  UserFacade userFacade = mock(UserFacade.class);
  ModelAndView modelAndView = mock(ModelAndView.class);
  given(userFacade.getTimezone()).willReturn("timezone X");

  //when
  new UserDataInterceptor(userFacade)
    .postHandle(null, null, null, modelAndView);

  //then
  verify(modelAndView).addObject("timezone", "timezone X"); 
}
----
This is of course wrong. The thing we really care about is whether +modelAndView+ contains certain data, not whether some methods of +modelAndView+ were called.

Below you can see an improved version of this test. It treates SUT more like a black box and only cares about its output.

----
@Test
public void shouldAddTimeZoneToModelAndView() {
  //given
  UserFacade userFacade = mock(UserFacade.class);
  ModelAndView modelAndView = new ModelAndView();
  given(userFacade.getTimezone()).willReturn("timezone X");

  //when
  new UserDataInterceptor(userFacade)
      .postHandle(null, null, null, modelAndView);

  //then
  assertThat(modelAndView).constains("timezone", "timezone X");
}
----
As you can see in the second version we use real object, not a test double. And this is fine, because +ModelAndView+ is not a service, but a value object, which should not be mocked. Howgh!

== Time Means Troubles (Always!)

----
public String getZendeskUrl(User user, String timestamp) {
    String name=user.getFullName();

    String url=baseUrl
        +"name="+URLEncoder.encode(name, "UTF-8")
        +"&timestamp="+timestamp;
    return url;
}

public String getZendeskUrl(User user) {
    Date date=new Date();
    Long time=(date.getTime()/1000); //convert ms to seconds
    String timestamp=time.toString();
    return getZendeskUrl(user, timestamp);
}
----

== TODO SRP for Tests
We are all familiar with Single Responsibility Principle{empty}pass:[]footnote:[SRP, see http://en.wikipedia.org/wiki/Single_responsibility_principle], which basically says that every class should take care of one thing. I think it is very valuable to think about tests in terms of SRP. In case of tests, each test method should conform to the following rule: '"Each test method should verify one scenario"'.

Why? Because:

* such test methods are pretty simple to understand,
* if they fail you know *exactly* which functionality of your software does not work.

NOTE: Writing tests which fullfil the SRP principle is very simple for unit tests, but not always applicable for other kind of tests.

Still, I observe it being breached frequently. Let us have a look at an example. It comes from a simple utility class, which was responsible for making sure that the phone prefix entered by users are valid.

----
@DataProvider
public Object[][] data() {
  return new Object[][] {
    {"48", true}, {"+48", true}, {"++48", true}, 
    {"+48503", true}, {"+4", false}, {"++4", false},
    {"", false}, {null, false}, {"  ", false}, };
}

@Test(dataProvider = "data")
public void testQueryVerification(String query, boolean expected) {
    assertEquals(expected, FieldVerifier.isValidQuery(query));
}
----
At the first sight, it may seem like the test is really focused on one thing (on the verification of query validity), but if you look closer you will see some alarming signals:

* its method name is quite generic, and it would be hard to use a '"should"' prefix (see <<sec_should>>),
* it has some kind of logic: assertions depends on passed +boolean+ flag (see <<sec_no_logic>>).

[[sec_no_logic]]
== Logic in Tests
TODO
avoid by using some features of your testing framework - annotations data provider instead of for loops (TODO ref) and concurrent with annotations

== Use Smart Values
* use of 2's power
* do not use same values, because you will not know if it works well or not (CW example XYZFactory constructor)

== What You Want To Test?
TODO integration DB test - adding user, checking if there is one - works fine with rollback (no it does not, because one day you need to add some users to the system, so there is always at least one in the database and it stops working), but then the need occurs to add a "fake" user for some specific purposes - your tests need to be rewritten. The thing is you verified the wrong thing. You verified if there is exactly one user in DB, when in fact what should be verified is whether the numbe of users in DB has increased by 1.

== Happy Path
The issue of writing '"happy path"' tests is so common, that it is probably the most popular antipattern of all.

But what is a '"happy path"' testing anyway? For our purpose, it is enough to say that '"happy paths"' tests cover the simplest, even obvious, scenarios. It is like testing that our calculator can add +2+ and +2+, and that if you add user +John Doe+ to the database then you can found it there.

Let me stress one thing though. Such tests are not evil. There is nothing wrong with having them. On the contrary *there are essential*. Why? Because the scenarios they cover are the most important for you application. If your application does not handle such simple cases than it is useless. So the problem is not with writing such tests, but with the fact that they are often the only tests there are. This of course leads to the problems as soon as the real data comes in - suddenly it occurs that your calculator can't handle negative values, and that your database is not prepared to handle a surname longer than 20 characters or so. And your clients will notice it really soon.

TODO

== Naming is the King
This one is interesting, because we are usually very good at giving good names to methods and variables in production code. However when it comes to tests, we do it much worse. I suspect this is because in tests it is quite normal to have few objects of them same type which results in simple naming schemas like +productA+, +productB+, +productC+ etc. Sometimes it is good enough, but sometimes it could be well improved.

TODO

* user_1
* domain1

TODO

* do not make me learn the API
  * statics
  * builders

[[sec_should]]
=== Should is Better than Test
* the names of test methods
TODO in relation to "test behaviour not methods"

== Avoid Assertions using Private Methods
So you have a test scenario. It is a complex integration test which covers some important functionality. At the end of it you write some assertions to make sure that the data in the database are in expected state. It can look like this:

----
TODO

test with assertState but inline, and with less assertions.
----

So far so good. Now you write other tests. You soon discover that the assertions are very similar to the ones you have already written. '"Reuse is good"' you think and you do the most obvious thing - you create a private method and call it from both tests (and later from more tests).

And all is good, at least for some time. You write more tests still reusing this one assertion method. But soon you encounter problems. The assertions in the method are not exactly what you need in some particular test. They are very close to what you want, but not exactly. '"Reuse is good"' you say and make your private method more generic so it can handle more test cases. The result is presented below:

----
TODO normal assertState() method
----

Oh my, this has come too far... Now we have a monster-method which verifies every possible scenario! It takes TODO almost 10 parameters and has some logic. This is unmaintainable. You can't read your tests now and understand what scenario is really tested (without doing some serious investigations).

And what can be done about it.

* assertions
* setup

TODO rule of a thumb: whenever your assertions are growing beyond your safety limit (2 lines? 3 lines? 5 lines? I do not know, it is *yours safety limit* not mine) then introduce custom assertions instead. In fact, it seems reasonable to come with custom assertions for the main objects from your domain (because you will probably have many tests which involve them - both on unit and integration level).

And how to come with good custom assertions method? My trick is simply to write it so I am happy while reading this. So, without actully having any custom assertion implemented I write a line like this in my test code:

----
assertThat(client).isVip().and().hasDiscount(0.2)twitter
----

And then I implement it. Of course sometimes it happens that I need to deviate from the original plan and that the resulting fluent interface differs in details, but usually I'm able to come quite close to what I imagined.

=== Cost of Custom Assertions
There is a question which could be asked by an inquisitive reader. Custom assertions contain some code, even some logic. How can I be sure that there is no mistake there?

Ok, I admit. I do not test my assertions. This comes from the fact that the logic of custom assertions is very limited (if any at all).

TODO question how do you test your custom assertions? I don't. 1) It is moving lines from test (which wouldn't have been tested anyway) 2) they are dead simple

== Excessive SetUp
TODO merge with previous one?

builders

== Things to Remember

* Single Responsibility Principle for tests
* Names are important!
* Test behaviour, not methods!
* Ask yourself "what is the purpose of this test?", "what is really important?"
* Think about readability (just say it)
* Test-last does not work
* Private methods for assertions do not work
* Tests work only if you treat them as first-class citizens

== Know Your Tool
TODO

== Randomness
TODO ref Jaroslav Tulach
TODO ref Carrot Search

== Help
Send me your tests!

== TODO Book Teaser
TODO

TODO cover img

http://practicalunittesting.com
